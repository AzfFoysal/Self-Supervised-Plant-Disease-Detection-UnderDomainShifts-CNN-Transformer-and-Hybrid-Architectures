"""Training entrypoint.

Trains one of:
  - ResNet50 (ImageNet-pretrained)
  - ViT-Base DINOv2 (timm pretrained)
  - Hybrid CNNâ€“ViT (ResNet50 + ViT-Base DINOv2 with softmax fusion weights)

Reproducibility (thesis-aligned):
  - PlantVillage is assumed to be placed at: data/PlantVillage/raw/<class>/*.jpg
  - Splits are generated by scripts/create_splits.py into a JSON (seed=42 by default)
  - This script *only* trains using those split indices (no reliance on pre-split folders).
  - Low-label training uses stratified subsets (10% or 25%) drawn from TRAIN split only.

Run examples:
  python scripts/create_splits.py --data_dir data --seed 42
  python scripts/train.py --model hybrid --data_dir data --seed 42 --amp --grad_ckpt
  python scripts/train.py --model resnet --data_dir data --seed 42 --subset 0.25
"""

from __future__ import annotations

import argparse
import json
import os
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset
from torchvision import datasets

from models.resnet50_model import ResNet50Classifier
from models.vit_dino_model import ViTClassifier
from models.hybrid_cnn_vit_model import HybridCNNViTModel
from utils.augmentation import (
    train_transform_cnn,
    train_transform_vit,
    val_transform_cnn,
    val_transform_vit,
)
from utils.dataset_prep import HybridDataset
from utils.repro import set_global_seed
from utils.stratified_split import stratified_subsample


@dataclass
class Split:
    train_idx: List[int]
    val_idx: List[int]
    test_idx: List[int]
    classes: List[str]


class TransformFromSamples(Dataset):
    """Dataset that loads (path,label) from a .samples list and applies a transform."""

    def __init__(self, samples: List[Tuple[str, int]], loader, transform=None):
        self.samples = samples
        self.loader = loader
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        return img, label


def load_split(split_json: str) -> Split:
    with open(split_json, "r", encoding="utf-8") as f:
        obj = json.load(f)
    return Split(
        train_idx=obj["train_idx"],
        val_idx=obj["val_idx"],
        test_idx=obj["test_idx"],
        classes=obj["classes"],
    )


def get_optimizer(model: nn.Module, model_type: str) -> optim.Optimizer:
    """AdamW with thesis-style per-module learning rates."""
    if model_type == "resnet":
        params = [
            {"params": model.model.parameters(), "lr": 5e-4},
        ]
    elif model_type == "vit":
        params = [
            {"params": model.classifier.parameters(), "lr": 5e-4},
            {"params": model.backbone.parameters(), "lr": 5e-5},
        ]
    elif model_type == "hybrid":
        params = [
            # heads / fusion
            {"params": list(model.fc1.parameters()) + list(model.fc2.parameters()), "lr": 1e-4},
            {"params": [model.fusion_logits], "lr": 1e-4},
            # backbones
            {"params": model.cnn_branch.parameters(), "lr": 5e-5},
            {"params": model.vit_branch.parameters(), "lr": 5e-6},
        ]
    else:
        params = [{"params": model.parameters(), "lr": 1e-4}]
    return optim.AdamW(params, weight_decay=1e-4)


def maybe_enable_grad_checkpointing(model: nn.Module, model_type: str, enabled: bool) -> None:
    if not enabled:
        return
    # timm models often expose set_grad_checkpointing
    if model_type == "vit":
        if hasattr(model.backbone, "set_grad_checkpointing"):
            model.backbone.set_grad_checkpointing(True)
    elif model_type == "hybrid":
        if hasattr(model.vit_branch, "set_grad_checkpointing"):
            model.vit_branch.set_grad_checkpointing(True)


def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    scaler: torch.cuda.amp.GradScaler,
    use_amp: bool,
    model_type: str,
    device: torch.device,
) -> Tuple[float, float]:
    model.train()
    loss_sum, correct, total = 0.0, 0, 0
    for batch in loader:
        optimizer.zero_grad(set_to_none=True)

        with torch.cuda.amp.autocast(enabled=use_amp):
            if model_type == "hybrid":
                (x_cnn, x_vit), y = batch
                x_cnn = x_cnn.to(device, non_blocking=True)
                x_vit = x_vit.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)
                logits = model(x_cnn, x_vit)
            else:
                x, y = batch
                x = x.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)
                logits = model(x)
            loss = criterion(logits, y)

        if use_amp:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        loss_sum += float(loss.detach().cpu()) * y.size(0)
        preds = logits.argmax(dim=1)
        correct += int((preds == y).sum().item())
        total += int(y.size(0))

    return loss_sum / max(1, total), correct / max(1, total)


@torch.no_grad()
def eval_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    use_amp: bool,
    model_type: str,
    device: torch.device,
) -> Tuple[float, float]:
    model.eval()
    loss_sum, correct, total = 0.0, 0, 0
    for batch in loader:
        with torch.cuda.amp.autocast(enabled=use_amp):
            if model_type == "hybrid":
                (x_cnn, x_vit), y = batch
                x_cnn = x_cnn.to(device, non_blocking=True)
                x_vit = x_vit.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)
                logits = model(x_cnn, x_vit)
            else:
                x, y = batch
                x = x.to(device, non_blocking=True)
                y = y.to(device, non_blocking=True)
                logits = model(x)
            loss = criterion(logits, y)

        loss_sum += float(loss.detach().cpu()) * y.size(0)
        preds = logits.argmax(dim=1)
        correct += int((preds == y).sum().item())
        total += int(y.size(0))
    return loss_sum / max(1, total), correct / max(1, total)


def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    model_type: str,
    epochs: int,
    use_amp: bool,
    grad_ckpt: bool,
) -> Tuple[nn.Module, Dict[str, List[float]]]:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    maybe_enable_grad_checkpointing(model, model_type, grad_ckpt)

    optimizer = get_optimizer(model, model_type)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
    criterion = nn.CrossEntropyLoss()
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

    best_val = float("inf")
    best_state = None
    patience, wait = 5, 0

    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

    for epoch in range(1, epochs + 1):
        tr_loss, tr_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler, use_amp, model_type, device
        )
        va_loss, va_acc = eval_one_epoch(model, val_loader, criterion, use_amp, model_type, device)
        scheduler.step()

        history["train_loss"].append(tr_loss)
        history["val_loss"].append(va_loss)
        history["train_acc"].append(tr_acc)
        history["val_acc"].append(va_acc)

        print(
            f"Epoch {epoch:03d}/{epochs} | "
            f"train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | "
            f"val_loss={va_loss:.4f} val_acc={va_acc:.4f}"
        )

        if va_loss < best_val:
            best_val = va_loss
            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                print("Early stopping triggered.")
                break

    if best_state is not None:
        model.load_state_dict(best_state)

    return model, history


def build_dataloaders(
    data_dir: str,
    split: Split,
    model_type: str,
    seed: int,
    subset: float,
    batch_size: int,
) -> Tuple[DataLoader, DataLoader, int]:
    """Create train/val loaders from raw dataset + split indices."""
    raw_dir = os.path.join(data_dir, "PlantVillage", "raw")
    base_raw = datasets.ImageFolder(raw_dir)  # no transform here

    # Training subset (optional, stratified) from TRAIN indices only.
    train_indices = split.train_idx
    if subset < 1.0:
        # create a temporary dataset-like object with targets aligned to train_indices
        tmp = Subset(base_raw, train_indices)
        # Subset doesn't expose targets; use base_raw.targets and map
        class Tmp:
            targets = [base_raw.targets[i] for i in train_indices]

        tmp.targets = Tmp.targets  # type: ignore
        sub_idx_local = stratified_subsample(tmp, subset, seed=seed)
        train_indices = [train_indices[i] for i in sub_idx_local]

    num_classes = len(split.classes)

    if model_type == "hybrid":
        train_ds_full = HybridDataset(
            base_raw, transform_cnn=train_transform_cnn, transform_vit=train_transform_vit
        )
        val_ds_full = HybridDataset(
            base_raw, transform_cnn=val_transform_cnn, transform_vit=val_transform_vit
        )
        train_ds = Subset(train_ds_full, train_indices)
        val_ds = Subset(val_ds_full, split.val_idx)
    else:
        tf_train = train_transform_vit if model_type == "vit" else train_transform_cnn
        tf_val = val_transform_vit if model_type == "vit" else val_transform_cnn
        # materialize samples list for indices to avoid ImageFolder transform mismatch
        train_samples = [base_raw.samples[i] for i in train_indices]
        val_samples = [base_raw.samples[i] for i in split.val_idx]
        train_ds = TransformFromSamples(train_samples, base_raw.loader, transform=tf_train)
        val_ds = TransformFromSamples(val_samples, base_raw.loader, transform=tf_val)

    train_loader = DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
    )
    return train_loader, val_loader, num_classes


def main():
    parser = argparse.ArgumentParser(description="Train Plant Disease models (thesis reproduction)")
    parser.add_argument("--model", choices=["resnet", "vit", "hybrid"], required=True)
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument(
        "--split_json",
        type=str,
        default=os.path.join("data", "splits", "plantvillage_split_seed42.json"),
    )
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument(
        "--subset",
        type=float,
        default=1.0,
        help="Fraction of TRAIN split to use (1.0, 0.25, 0.10)",
    )
    parser.add_argument("--amp", action="store_true", help="Enable mixed precision")
    parser.add_argument("--grad_ckpt", action="store_true", help="Enable grad checkpointing (ViT)")
    parser.add_argument("--batch_size", type=int, default=0, help="Override default batch size")
    args = parser.parse_args()

    set_global_seed(args.seed)

    split = load_split(args.split_json)

    default_bs = 64 if args.model in ("resnet", "hybrid") else 32
    batch_size = args.batch_size if args.batch_size > 0 else default_bs

    train_loader, val_loader, num_classes = build_dataloaders(
        args.data_dir, split, args.model, args.seed, args.subset, batch_size
    )

    if args.model == "resnet":
        model = ResNet50Classifier(num_classes=num_classes, pretrained=True)
    elif args.model == "vit":
        model = ViTClassifier(num_classes=num_classes, pretrained=True)
    else:
        model = HybridCNNViTModel(num_classes=num_classes)

    model, history = train_model(
        model,
        train_loader,
        val_loader,
        model_type=args.model,
        epochs=args.epochs,
        use_amp=args.amp,
        grad_ckpt=args.grad_ckpt,
    )

    os.makedirs("checkpoints", exist_ok=True)
    suffix = "full" if args.subset >= 1.0 else f"{int(args.subset*100)}pc"
    ckpt_path = os.path.join("checkpoints", f"{args.model}_{suffix}_seed{args.seed}.pth")
    torch.save(model.state_dict(), ckpt_path)
    print(f"Saved checkpoint: {ckpt_path}")


if __name__ == "__main__":
    main()
